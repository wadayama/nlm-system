#!/usr/bin/env python
"""Speed comparison between gpt-5-nano and gpt-oss:20b"""

import time
from nlm_interpreter import NLMSession
import statistics


def measure_execution_time(session, command, runs=3):
    """Measure average execution time over multiple runs"""
    times = []
    
    for i in range(runs):
        start = time.time()
        try:
            result = session.execute(command)
            elapsed = time.time() - start
            times.append(elapsed)
            print(f"      Run {i+1}: {elapsed:.3f}s")
        except Exception as e:
            print(f"      Run {i+1}: Failed - {e}")
            continue
    
    if times:
        return {
            "avg": statistics.mean(times),
            "min": min(times),
            "max": max(times),
            "median": statistics.median(times),
            "runs": len(times)
        }
    return None


def run_speed_comparison():
    """Compare speed between gpt-5-nano and gpt-oss:20b"""
    print("="*70)
    print("ğŸ SPEED COMPARISON: gpt-5-nano vs gpt-oss:20b")
    print("="*70)
    
    # Test cases
    test_cases = [
        ("Simple assignment", "Set {{test_var}} to 'Speed Test'"),
        ("Math calculation", "Calculate 123 * 456 and save to {{result}}"),
        ("Multi-variable", "Set {{name}} to 'Test' and {{value}} to 100"),
        ("Japanese processing", "{{ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸}}ã‚’'é«˜é€Ÿãƒ†ã‚¹ãƒˆ'ã«è¨­å®šã—ã¦ãã ã•ã„")
    ]
    
    results = {}
    
    # Test gpt-5-nano
    print("\nğŸš€ Testing gpt-5-nano (OpenAI Cloud)")
    print("-" * 50)
    session_nano = NLMSession(model="gpt-5-nano", namespace="speed_test_nano")
    nano_times = []
    
    for test_name, command in test_cases:
        print(f"\n   ğŸ“ {test_name}:")
        print(f"      Command: {command[:50]}...")
        result = measure_execution_time(session_nano, command, runs=3)
        if result:
            nano_times.append(result['avg'])
            print(f"      Average: {result['avg']:.3f}s (min: {result['min']:.3f}s, max: {result['max']:.3f}s)")
    
    # Test gpt-oss:20b
    print("\n\nğŸ–¥ï¸  Testing gpt-oss:20b (Local)")
    print("-" * 50)
    session_local = NLMSession(model="gpt-oss:20b", namespace="speed_test_local")
    local_times = []
    
    for test_name, command in test_cases:
        print(f"\n   ğŸ“ {test_name}:")
        print(f"      Command: {command[:50]}...")
        result = measure_execution_time(session_local, command, runs=3)
        if result:
            local_times.append(result['avg'])
            print(f"      Average: {result['avg']:.3f}s (min: {result['min']:.3f}s, max: {result['max']:.3f}s)")
    
    # Summary
    print("\n\n" + "="*70)
    print("ğŸ“Š PERFORMANCE SUMMARY")
    print("="*70)
    
    if nano_times and local_times:
        nano_avg = statistics.mean(nano_times)
        local_avg = statistics.mean(local_times)
        
        print(f"\nğŸš€ gpt-5-nano (OpenAI):")
        print(f"   Average response time: {nano_avg:.3f}s")
        print(f"   Min: {min(nano_times):.3f}s")
        print(f"   Max: {max(nano_times):.3f}s")
        
        print(f"\nğŸ–¥ï¸  gpt-oss:20b (Local):")
        print(f"   Average response time: {local_avg:.3f}s")
        print(f"   Min: {min(local_times):.3f}s")
        print(f"   Max: {max(local_times):.3f}s")
        
        # Speed comparison
        speedup = local_avg / nano_avg
        print(f"\nâš¡ SPEED COMPARISON:")
        print(f"   gpt-5-nano is {speedup:.1f}x faster than gpt-oss:20b")
        
        if speedup > 3:
            print(f"   ğŸ¯ Significant performance advantage!")
            print(f"   ğŸ’¡ Recommendation: Use gpt-5-nano for time-critical tasks")
        elif speedup > 1.5:
            print(f"   âœ… Notable performance improvement")
            print(f"   ğŸ’¡ Recommendation: Consider gpt-5-nano for better responsiveness")
        else:
            print(f"   ğŸ¤ Comparable performance")
            print(f"   ğŸ’¡ Recommendation: Choose based on cost and privacy requirements")
        
        # Detailed comparison table
        print(f"\nğŸ“ˆ DETAILED COMPARISON:")
        print(f"   {'Task':<25} {'gpt-5-nano':<12} {'gpt-oss:20b':<12} {'Speedup':<10}")
        print(f"   {'-'*25} {'-'*12} {'-'*12} {'-'*10}")
        
        for i, (test_name, _) in enumerate(test_cases):
            if i < len(nano_times) and i < len(local_times):
                speedup_task = local_times[i] / nano_times[i]
                print(f"   {test_name:<25} {nano_times[i]:<12.3f} {local_times[i]:<12.3f} {speedup_task:<10.1f}x")
        
        # Cost-benefit analysis
        print(f"\nğŸ’° COST-BENEFIT ANALYSIS:")
        print(f"   gpt-5-nano:")
        print(f"      âœ… {speedup:.1f}x faster response")
        print(f"      âŒ API costs apply")
        print(f"      âŒ Requires internet connection")
        print(f"      âœ… No local GPU/CPU load")
        
        print(f"\n   gpt-oss:20b:")
        print(f"      âœ… Free (no API costs)")
        print(f"      âœ… Works offline")
        print(f"      âœ… Complete privacy")
        print(f"      âŒ {speedup:.1f}x slower")
        print(f"      âŒ Uses local resources")
    
    print("\n" + "="*70)


if __name__ == "__main__":
    try:
        run_speed_comparison()
    except KeyboardInterrupt:
        print("\nâš ï¸ Test interrupted by user")
    except Exception as e:
        print(f"\nâŒ Test failed: {e}")
        import traceback
        traceback.print_exc()