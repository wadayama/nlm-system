#!/usr/bin/env python3
"""Test conditional logic capabilities of NLM system

This test suite specifically targets conditional branching,
a known weak point in LLM-based macro processing.
Based on patterns from macro.md documentation.
"""

from nlm_interpreter import NLMSession
import json
from pathlib import Path
import time

# Color codes for output
GREEN = '\033[92m'
RED = '\033[91m'
CYAN = '\033[96m'  # Changed from YELLOW for better readability
BLUE = '\033[94m'
RESET = '\033[0m'


def test_simple_if_then(session):
    """Test simple if-then conditional"""
    test_name = "Simple if-then"
    
    # Clear previous state
    session.clear_local()
    
    # Test 1: Score >= 80 (should be åˆæ ¼)
    session.save("score", "85")
    result = session.execute(
        "{{score}}ãŒ80ä»¥ä¸Šã®å ´åˆã¯ã€Œåˆæ ¼ã€ã€ãã†ã§ãªã„å ´åˆã¯ã€Œä¸åˆæ ¼ã€ã‚’{{result}}ã«ä¿å­˜ã—ã¦ãã ã•ã„"
    )
    actual_result = session.get("result")
    expected = "åˆæ ¼"
    success1 = actual_result == expected
    
    # Test 2: Score < 80 (should be ä¸åˆæ ¼)
    session.save("score", "75")
    result = session.execute(
        "{{score}}ãŒ80ä»¥ä¸Šã®å ´åˆã¯ã€Œåˆæ ¼ã€ã€ãã†ã§ãªã„å ´åˆã¯ã€Œä¸åˆæ ¼ã€ã‚’{{result}}ã«ä¿å­˜ã—ã¦ãã ã•ã„"
    )
    actual_result = session.get("result")
    expected = "ä¸åˆæ ¼"
    success2 = actual_result == expected
    
    return success1 and success2, test_name, f"Test1: {success1}, Test2: {success2}"


def test_multi_level_conditions(session):
    """Test multi-level if-elif-else conditions"""
    test_name = "Multi-level conditions"
    
    session.clear_local()
    
    # Test with score = 85 (should be å„ªç§€)
    session.save("score", "85")
    result = session.execute("""
{{score}}ã«åŸºã¥ã„ã¦è©•ä¾¡ã‚’æ±ºå®šã—ã¦ãã ã•ã„ï¼š
- 80ç‚¹ä»¥ä¸Šã®å ´åˆã¯ã€Œå„ªç§€ã€
- 60ç‚¹ä»¥ä¸Š80ç‚¹æœªæº€ã®å ´åˆã¯ã€Œè‰¯å¥½ã€
- 60ç‚¹æœªæº€ã®å ´åˆã¯ã€Œè¦æ”¹å–„ã€
çµæœã‚’{{evaluation}}ã«ä¿å­˜ã—ã¦ãã ã•ã„
""")
    
    actual = session.get("evaluation")
    expected = "å„ªç§€"
    success1 = actual == expected
    
    # Test with score = 70 (should be è‰¯å¥½)
    session.save("score", "70")
    result = session.execute("""
{{score}}ã«åŸºã¥ã„ã¦è©•ä¾¡ã‚’æ±ºå®šã—ã¦ãã ã•ã„ï¼š
- 80ç‚¹ä»¥ä¸Šã®å ´åˆã¯ã€Œå„ªç§€ã€
- 60ç‚¹ä»¥ä¸Š80ç‚¹æœªæº€ã®å ´åˆã¯ã€Œè‰¯å¥½ã€
- 60ç‚¹æœªæº€ã®å ´åˆã¯ã€Œè¦æ”¹å–„ã€
çµæœã‚’{{evaluation}}ã«ä¿å­˜ã—ã¦ãã ã•ã„
""")
    
    actual = session.get("evaluation")
    expected = "è‰¯å¥½"
    success2 = actual == expected
    
    # Test with score = 50 (should be è¦æ”¹å–„)
    session.save("score", "50")
    result = session.execute("""
{{score}}ã«åŸºã¥ã„ã¦è©•ä¾¡ã‚’æ±ºå®šã—ã¦ãã ã•ã„ï¼š
- 80ç‚¹ä»¥ä¸Šã®å ´åˆã¯ã€Œå„ªç§€ã€
- 60ç‚¹ä»¥ä¸Š80ç‚¹æœªæº€ã®å ´åˆã¯ã€Œè‰¯å¥½ã€
- 60ç‚¹æœªæº€ã®å ´åˆã¯ã€Œè¦æ”¹å–„ã€
çµæœã‚’{{evaluation}}ã«ä¿å­˜ã—ã¦ãã ã•ã„
""")
    
    actual = session.get("evaluation")
    expected = "è¦æ”¹å–„"
    success3 = actual == expected
    
    return (success1 and success2 and success3), test_name, f"85â†’å„ªç§€:{success1}, 70â†’è‰¯å¥½:{success2}, 50â†’è¦æ”¹å–„:{success3}"


def test_compound_conditions(session):
    """Test compound conditions with AND/OR logic"""
    test_name = "Compound conditions"
    
    session.clear_local()
    
    # Test: Cold and Rainy (should be å¤–å‡ºæ³¨æ„)
    session.save("temperature", "15")
    session.save("weather", "é›¨")
    result = session.execute("""
{{temperature}}ãŒ20åº¦æœªæº€ã§{{weather}}ãŒã€Œé›¨ã€ã®å ´åˆã¯ã€Œå¤–å‡ºæ³¨æ„ã€ã€
ãã†ã§ãªã„å ´åˆã¯ã€Œé€šå¸¸å¤–å‡ºã€ã‚’{{advice}}ã«ä¿å­˜ã—ã¦ãã ã•ã„
""")
    
    actual = session.get("advice")
    expected = "å¤–å‡ºæ³¨æ„"
    success1 = actual == expected
    
    # Test: Warm and Rainy (should be é€šå¸¸å¤–å‡º)
    session.save("temperature", "25")
    session.save("weather", "é›¨")
    result = session.execute("""
{{temperature}}ãŒ20åº¦æœªæº€ã§{{weather}}ãŒã€Œé›¨ã€ã®å ´åˆã¯ã€Œå¤–å‡ºæ³¨æ„ã€ã€
ãã†ã§ãªã„å ´åˆã¯ã€Œé€šå¸¸å¤–å‡ºã€ã‚’{{advice}}ã«ä¿å­˜ã—ã¦ãã ã•ã„
""")
    
    actual = session.get("advice")
    expected = "é€šå¸¸å¤–å‡º"
    success2 = actual == expected
    
    return (success1 and success2), test_name, f"Cold&Rainâ†’å¤–å‡ºæ³¨æ„:{success1}, Warm&Rainâ†’é€šå¸¸å¤–å‡º:{success2}"


def test_variable_swap(session):
    """Test variable swapping - a known difficult case"""
    test_name = "Variable swap"
    
    session.clear_local()
    
    # Initialize variables
    session.save("a", "apple")
    session.save("b", "banana")
    
    # Request swap
    result = session.execute("""
{{a}}ã¨{{b}}ã®å€¤ã‚’å…¥ã‚Œæ›¿ãˆã¦ãã ã•ã„ã€‚
ä¸€æ™‚å¤‰æ•°{{temp}}ã‚’ä½¿ã£ã¦ã€ä»¥ä¸‹ã®æ‰‹é †ã§å®Ÿè¡Œï¼š
1. {{a}}ã®å€¤ã‚’{{temp}}ã«ä¿å­˜
2. {{b}}ã®å€¤ã‚’{{a}}ã«ä¿å­˜
3. {{temp}}ã®å€¤ã‚’{{b}}ã«ä¿å­˜
""")
    
    # Check if swap was successful
    new_a = session.get("a")
    new_b = session.get("b")
    
    success = (new_a == "banana" and new_b == "apple")
    
    return success, test_name, f"a={new_a} (expect: banana), b={new_b} (expect: apple)"


def test_string_matching(session):
    """Test string matching conditions"""
    test_name = "String matching"
    
    session.clear_local()
    
    # Test with status = "active"
    session.save("status", "active")
    result = session.execute("""
{{status}}ãŒã€Œactiveã€ã®å ´åˆã¯ã€Œã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒä¸­ã€ã€
ã€Œinactiveã€ã®å ´åˆã¯ã€Œã‚·ã‚¹ãƒ†ãƒ åœæ­¢ä¸­ã€ã€
ãã®ä»–ã®å ´åˆã¯ã€Œä¸æ˜ãªçŠ¶æ…‹ã€ã‚’{{message}}ã«ä¿å­˜ã—ã¦ãã ã•ã„
""")
    
    actual = session.get("message")
    expected = "ã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒä¸­"
    success1 = actual == expected
    
    # Test with status = "inactive"
    session.save("status", "inactive")
    result = session.execute("""
{{status}}ãŒã€Œactiveã€ã®å ´åˆã¯ã€Œã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒä¸­ã€ã€
ã€Œinactiveã€ã®å ´åˆã¯ã€Œã‚·ã‚¹ãƒ†ãƒ åœæ­¢ä¸­ã€ã€
ãã®ä»–ã®å ´åˆã¯ã€Œä¸æ˜ãªçŠ¶æ…‹ã€ã‚’{{message}}ã«ä¿å­˜ã—ã¦ãã ã•ã„
""")
    
    actual = session.get("message")
    expected = "ã‚·ã‚¹ãƒ†ãƒ åœæ­¢ä¸­"
    success2 = actual == expected
    
    return (success1 and success2), test_name, f"activeâ†’ç¨¼åƒä¸­:{success1}, inactiveâ†’åœæ­¢ä¸­:{success2}"


def test_numeric_comparisons(session):
    """Test various numeric comparison operators"""
    test_name = "Numeric comparisons"
    
    session.clear_local()
    results = []
    
    # Test greater than
    session.save("value", "15")
    result = session.execute(
        "{{value}}ãŒ10ã‚ˆã‚Šå¤§ãã„å ´åˆã¯ã€Œå¤§ã€ã€ãã†ã§ãªã„å ´åˆã¯ã€Œå°ã€ã‚’{{size}}ã«ä¿å­˜ã—ã¦ãã ã•ã„"
    )
    actual = session.get("size")
    results.append(actual == "å¤§")
    
    # Test less than or equal
    session.save("value", "10")
    result = session.execute(
        "{{value}}ãŒ10ä»¥ä¸‹ã®å ´åˆã¯ã€ŒOKã€ã€ãã†ã§ãªã„å ´åˆã¯ã€ŒNGã€ã‚’{{check}}ã«ä¿å­˜ã—ã¦ãã ã•ã„"
    )
    actual = session.get("check")
    results.append(actual == "OK")
    
    # Test equality
    session.save("value", "100")
    result = session.execute(
        "{{value}}ãŒ100ã¨ç­‰ã—ã„å ´åˆã¯ã€Œä¸€è‡´ã€ã€ãã†ã§ãªã„å ´åˆã¯ã€Œä¸ä¸€è‡´ã€ã‚’{{match}}ã«ä¿å­˜ã—ã¦ãã ã•ã„"
    )
    actual = session.get("match")
    results.append(actual == "ä¸€è‡´")
    
    success = all(results)
    return success, test_name, f"GT:{results[0]}, LE:{results[1]}, EQ:{results[2]}"


def test_ambiguous_conditions(session):
    """Test handling of ambiguous/qualitative conditions"""
    test_name = "Ambiguous conditions"
    
    session.clear_local()
    
    # Test with "ååˆ†ã«é«˜ã„" (sufficiently high)
    session.save("score", "95")
    result = session.execute(
        "{{score}}ãŒååˆ†ã«é«˜ã„å ´åˆã¯ã€Œå„ªç§€èªå®šã€ã€ãã†ã§ãªã„å ´åˆã¯ã€Œé€šå¸¸ã€ã‚’{{certification}}ã«ä¿å­˜ã—ã¦ãã ã•ã„"
    )
    
    actual = session.get("certification")
    # Check if it made a reasonable decision (95 should be considered "high")
    success1 = actual == "å„ªç§€èªå®š"
    
    # Test with low score
    session.save("score", "30")
    result = session.execute(
        "{{score}}ãŒååˆ†ã«é«˜ã„å ´åˆã¯ã€Œå„ªç§€èªå®šã€ã€ãã†ã§ãªã„å ´åˆã¯ã€Œé€šå¸¸ã€ã‚’{{certification}}ã«ä¿å­˜ã—ã¦ãã ã•ã„"
    )
    
    actual = session.get("certification")
    success2 = actual == "é€šå¸¸"
    
    return (success1 and success2), test_name, f"95â†’å„ªç§€èªå®š:{success1}, 30â†’é€šå¸¸:{success2}"


def run_test_suite(model="gpt-5-mini", reasoning="low"):
    """Run all conditional logic tests"""
    
    print(f"\n{BLUE}{'='*60}{RESET}")
    print(f"{BLUE}ğŸ§ª Conditional Logic Test Suite{RESET}")
    print(f"{BLUE}Model: {model}{RESET}")
    print(f"{BLUE}Reasoning: {reasoning}{RESET}")
    print(f"{BLUE}{'='*60}{RESET}\n")
    
    # Initialize session with reasoning level
    session = NLMSession(namespace=f"cond_test_{model}", model=model, reasoning_effort=reasoning)
    
    # Define all tests
    tests = [
        test_simple_if_then,
        test_multi_level_conditions,
        test_compound_conditions,
        test_variable_swap,
        test_string_matching,
        test_numeric_comparisons,
        test_ambiguous_conditions
    ]
    
    results = []
    total_time = 0
    
    # Run each test
    for i, test_func in enumerate(tests, 1):
        print(f"{CYAN}Test {i}/{len(tests)}: {test_func.__name__}{RESET}")
        
        start_time = time.time()
        try:
            success, test_name, details = test_func(session)
            elapsed = time.time() - start_time
            total_time += elapsed
            
            if success:
                print(f"  {GREEN}âœ… PASSED{RESET} - {test_name}")
            else:
                print(f"  {RED}âŒ FAILED{RESET} - {test_name}")
            print(f"  Details: {details}")
            print(f"  Time: {elapsed:.2f}s")
            
            results.append({
                "test": test_name,
                "success": success,
                "details": details,
                "time": elapsed
            })
            
        except Exception as e:
            print(f"  {RED}âŒ ERROR{RESET} - {str(e)}")
            results.append({
                "test": test_func.__name__,
                "success": False,
                "details": f"Error: {str(e)}",
                "time": 0
            })
        
        print()
    
    # Summary
    successful = sum(1 for r in results if r["success"])
    total = len(results)
    success_rate = (successful / total) * 100 if total > 0 else 0
    
    print(f"{BLUE}{'='*60}{RESET}")
    print(f"{BLUE}ğŸ“Š Test Summary{RESET}")
    print(f"{BLUE}{'='*60}{RESET}")
    print(f"Model: {model}")
    print(f"Success Rate: {success_rate:.1f}% ({successful}/{total})")
    print(f"Total Time: {total_time:.2f}s")
    print(f"Average Time per Test: {total_time/total:.2f}s")
    
    # Detailed failure analysis
    failed_tests = [r for r in results if not r["success"]]
    if failed_tests:
        print(f"\n{RED}Failed Tests:{RESET}")
        for r in failed_tests:
            print(f"  - {r['test']}: {r['details']}")
    
    return results, success_rate


def compare_models(reasoning="low"):
    """Compare conditional logic performance across models"""
    
    print(f"\n{BLUE}{'='*60}{RESET}")
    print(f"{BLUE}ğŸ”¬ Model Comparison: Conditional Logic{RESET}")
    print(f"{BLUE}Reasoning Level: {reasoning}{RESET}")
    print(f"{BLUE}{'='*60}{RESET}\n")
    
    models_to_test = []
    
    # Check if local model is available
    try:
        session_local = NLMSession(namespace="test_local", model="gpt-oss:20b")
        session_local.execute("Test connection")
        models_to_test.append(("gpt-oss:20b", "Local"))
    except:
        print(f"{CYAN}âš ï¸  Local model not available{RESET}")
    
    # Add OpenAI model
    models_to_test.append(("gpt-5-mini", "OpenAI"))
    
    comparison_results = {}
    
    for model, label in models_to_test:
        print(f"\n{BLUE}Testing {label} Model: {model}{RESET}")
        results, success_rate = run_test_suite(model, reasoning)
        comparison_results[model] = {
            "label": label,
            "results": results,
            "success_rate": success_rate
        }
    
    # Comparison summary
    if len(comparison_results) > 1:
        print(f"\n{BLUE}{'='*60}{RESET}")
        print(f"{BLUE}ğŸ“ˆ Comparison Results{RESET}")
        print(f"{BLUE}{'='*60}{RESET}\n")
        
        for model, data in comparison_results.items():
            print(f"{data['label']} ({model}):")
            print(f"  Success Rate: {data['success_rate']:.1f}%")
            
            # Test-by-test comparison
            print(f"  Test Results:")
            for r in data['results']:
                status = "âœ…" if r['success'] else "âŒ"
                print(f"    {status} {r['test']}")
        
        # Identify common failures
        print(f"\n{CYAN}Common Challenges:{RESET}")
        all_tests = set()
        failed_by_model = {}
        
        for model, data in comparison_results.items():
            failed_by_model[model] = set()
            for r in data['results']:
                all_tests.add(r['test'])
                if not r['success']:
                    failed_by_model[model].add(r['test'])
        
        # Find tests that all models failed
        common_failures = set.intersection(*failed_by_model.values()) if failed_by_model else set()
        if common_failures:
            print(f"{RED}Tests failed by all models:{RESET}")
            for test in common_failures:
                print(f"  - {test}")
        else:
            print(f"{GREEN}No common failures across models{RESET}")
        
        # Find model-specific strengths
        print(f"\n{GREEN}Model-Specific Strengths:{RESET}")
        for model, data in comparison_results.items():
            passed = set(r['test'] for r in data['results'] if r['success'])
            unique_passes = passed
            for other_model in comparison_results:
                if other_model != model:
                    other_failed = failed_by_model[other_model]
                    unique_passes = unique_passes.intersection(other_failed)
            
            if unique_passes:
                print(f"{data['label']} uniquely passed:")
                for test in unique_passes:
                    print(f"  - {test}")


def main():
    """Main entry point"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Test conditional logic in NLM system")
    parser.add_argument("-m", "--model", default="gpt-5-mini", 
                       help="Model to test (gpt-5-mini, gpt-oss:20b, etc.)")
    parser.add_argument("-c", "--compare", action="store_true",
                       help="Compare performance across models")
    parser.add_argument("-r", "--reasoning", default="low",
                       choices=["low", "medium", "high"],
                       help="Reasoning effort level (low, medium, high)")
    
    args = parser.parse_args()
    
    if args.compare:
        compare_models(args.reasoning)
    else:
        run_test_suite(args.model, args.reasoning)


if __name__ == "__main__":
    main()