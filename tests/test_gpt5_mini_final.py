#!/usr/bin/env python
"""Test final optimization with gpt-5-mini: reasoning_effort='low' + verbosity='low'"""

import time
from nlm_interpreter import NLMSession
import statistics


def test_gpt5_mini_final():
    """Test combined reasoning_effort='low' and verbosity='low' with gpt-5-mini"""
    print("="*70)
    print("ğŸš€ GPT-5-MINI æœ€çµ‚æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ")
    print("reasoning_effort='low' + verbosity='low'")
    print("="*70)
    
    session = NLMSession(model="gpt-5-mini", namespace="mini_final_test")
    
    # Same test cases as gpt-5-nano for comparison
    test_cases = [
        ("Basic assignment", "Set {{name}} to 'Bob'"),
        ("Math calculation", "Calculate 18 * 25 and save to {{math}}"),
        ("Multi-variable", "Set {{x}} to 10 and {{y}} to 20"),
        ("Japanese", "{{å ´æ‰€}}ã‚’'éŠ€åº§'ã«è¨­å®šã—ã¦ãã ã•ã„"),
        ("Complex calc", "Calculate (15 + 5) * 3 and store in {{result}}"),
        ("String processing", "Save 'Final Test' to {{message}}"),
        ("No variable", "What is 6 + 4?"),
        ("Conditional", "If 8 > 5, set {{status}} to 'pass'")
    ]
    
    print(f"\nğŸ“Š GPT-5-MINI æœ€é©åŒ–å¾Œã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š:")
    print("-" * 50)
    
    times = []
    var_times = []
    no_var_times = []
    successful_tests = 0
    response_lengths = []
    
    for i, (test_name, command) in enumerate(test_cases, 1):
        print(f"\n{i}. {test_name}:")
        print(f"   Command: {command}")
        
        start = time.time()
        try:
            result = session.execute(command)
            elapsed = time.time() - start
            times.append(elapsed)
            response_lengths.append(len(result))
            
            # Categorize times
            if "{{" in command:
                var_times.append(elapsed)
            else:
                no_var_times.append(elapsed)
            
            print(f"   â±ï¸  Time: {elapsed:.3f}s")
            
            # Verify result
            if "{{" in command and "}}" in command:
                var_name = command[command.find("{{")+2:command.find("}}")]
                value = session.get(var_name)
                if value:
                    print(f"   âœ… Variable: {var_name} = '{value}'")
                    successful_tests += 1
                else:
                    print(f"   âŒ Variable not set properly")
            else:
                print(f"   ğŸ“ Response: {result[:80]}...")
                if result and len(result) > 0:
                    successful_tests += 1
            
            # Show response length for verbosity analysis
            print(f"   ğŸ“ Response length: {len(result)} chars")
                
        except Exception as e:
            print(f"   âŒ Error: {e}")
    
    # Performance analysis
    if times:
        print("\n" + "="*70)
        print("ğŸ“ˆ GPT-5-MINI æœ€çµ‚æœ€é©åŒ–çµæœ:")
        print("="*70)
        
        avg_time = statistics.mean(times)
        success_rate = (successful_tests / len(test_cases)) * 100
        avg_response_length = statistics.mean(response_lengths) if response_lengths else 0
        
        print(f"  ãƒ†ã‚¹ãƒˆæˆåŠŸç‡: {successful_tests}/{len(test_cases)} ({success_rate:.1f}%)")
        print(f"  å¹³å‡å®Ÿè¡Œæ™‚é–“: {avg_time:.3f}s")
        print(f"  æœ€é€Ÿ: {min(times):.3f}s")
        print(f"  æœ€é…: {max(times):.3f}s")
        print(f"  ä¸­å¤®å€¤: {statistics.median(times):.3f}s")
        print(f"  å¹³å‡å¿œç­”é•·: {avg_response_length:.0f} chars")
        
        if var_times:
            print(f"\n  å¤‰æ•°ã‚ã‚Šæ“ä½œ:")
            print(f"    å¹³å‡: {statistics.mean(var_times):.3f}s")
            print(f"    æœ€é€Ÿ: {min(var_times):.3f}s")
            print(f"    æœ€é…: {max(var_times):.3f}s")
        
        if no_var_times:
            print(f"\n  å¤‰æ•°ãªã—æ“ä½œ:")
            print(f"    å¹³å‡: {statistics.mean(no_var_times):.3f}s")
        
        # Model comparison
        print("\n" + "="*70)
        print("ğŸ†š GPT-5-NANO vs GPT-5-MINI æ¯”è¼ƒ:")
        print("="*70)
        
        # gpt-5-nano results from previous test
        nano_results = {
            "avg_time": 4.085,
            "success_rate": 100.0,
            "var_avg": 4.462,
            "no_var_avg": 1.444
        }
        
        print(f"{'Metric':<20} {'GPT-5-NANO':<15} {'GPT-5-MINI':<15} {'Winner'}")
        print("-" * 65)
        
        # Overall time
        nano_faster = nano_results["avg_time"] < avg_time
        time_winner = "NANO ğŸ†" if nano_faster else "MINI ğŸ†"
        print(f"{'Average Time':<20} {nano_results['avg_time']:<15.3f}s {avg_time:<15.3f}s {time_winner}")
        
        # Success rate
        success_winner = "NANO ğŸ†" if nano_results["success_rate"] > success_rate else "MINI ğŸ†" if success_rate > nano_results["success_rate"] else "TIE ğŸ¤"
        print(f"{'Success Rate':<20} {nano_results['success_rate']:<15.1f}% {success_rate:<15.1f}% {success_winner}")
        
        # Variable operations
        if var_times:
            var_avg = statistics.mean(var_times)
            var_winner = "NANO ğŸ†" if nano_results["var_avg"] < var_avg else "MINI ğŸ†"
            print(f"{'Variable Ops':<20} {nano_results['var_avg']:<15.3f}s {var_avg:<15.3f}s {var_winner}")
        
        # No variable operations
        if no_var_times:
            no_var_avg = statistics.mean(no_var_times)
            no_var_winner = "NANO ğŸ†" if nano_results["no_var_avg"] < no_var_avg else "MINI ğŸ†"
            print(f"{'No Variable Ops':<20} {nano_results['no_var_avg']:<15.3f}s {no_var_avg:<15.3f}s {no_var_winner}")
        
        # Overall performance rating
        print(f"\nğŸ† ç·åˆè©•ä¾¡:")
        if success_rate >= 95 and avg_time < 3:
            mini_rating = "ğŸš€ Excellent"
        elif success_rate >= 90 and avg_time < 5:
            mini_rating = "âœ… Very Good"
        elif success_rate >= 80 and avg_time < 8:
            mini_rating = "ğŸ‘ Good"
        else:
            mini_rating = "âš ï¸ Needs Improvement"
        
        print(f"  GPT-5-MINI: {mini_rating}")
        print(f"  GPT-5-NANO: âœ… Very Good (å‚è€ƒ)")
        
        # Recommendation
        print(f"\nğŸ’¡ æ¨å¥¨äº‹é …:")
        if avg_time < nano_results["avg_time"] and success_rate >= nano_results["success_rate"]:
            print(f"  ğŸ¯ GPT-5-MINI ãŒç·åˆçš„ã«å„ªç§€")
            print(f"     é€Ÿåº¦ã¨å“è³ªã®ä¸¡æ–¹ã§å„ªä½æ€§ã‚’ç¤ºã™")
        elif success_rate >= 95:
            print(f"  âœ… ä¸¡ãƒ¢ãƒ‡ãƒ«ã¨ã‚‚å®Ÿç”¨ãƒ¬ãƒ™ãƒ«")
            print(f"     ç”¨é€”ã«å¿œã˜ã¦é¸æŠå¯èƒ½")
            if avg_time < nano_results["avg_time"]:
                print(f"     é€Ÿåº¦é‡è¦–: GPT-5-MINI")
            else:
                print(f"     ãƒãƒ©ãƒ³ã‚¹é‡è¦–: ä¸¡æ–¹ã¨ã‚‚è‰¯å¥½")
        else:
            print(f"  âš ï¸ å“è³ªã®ç¢ºèªãŒå¿…è¦")
        
        # Historical comparison for GPT-5-MINI
        print("\n" + "="*70)
        print("ğŸ“Š GPT-5-MINI è¨­å®šåˆ¥æ¯”è¼ƒ:")
        print("="*70)
        
        # Previous gpt-5-mini results (if available from earlier tests)
        configurations = [
            ("Original (ãªã—)", 11.3, 100),  # Estimated
            ("reasoning_effort='low'", 5.5, 100),  # From previous test
            ("æœ€çµ‚æœ€é©åŒ– (low+verbosity)", avg_time, success_rate)
        ]
        
        print(f"{'è¨­å®š':<30} {'æ™‚é–“':<8} {'å“è³ª':<8} {'è©•ä¾¡'}")
        print("-" * 60)
        
        for config_name, time_val, quality_val in configurations:
            if time_val < 3:
                time_rating = "ğŸš€"
            elif time_val < 6:
                time_rating = "âœ…"
            elif time_val < 10:
                time_rating = "âš ï¸"
            else:
                time_rating = "ğŸ¢"
            
            if quality_val >= 95:
                quality_rating = "ğŸ¯"
            elif quality_val >= 80:
                quality_rating = "âœ…"
            elif quality_val >= 50:
                quality_rating = "âš ï¸"
            else:
                quality_rating = "âŒ"
            
            print(f"{config_name:<30} {time_val:<8.1f}s {quality_val:<8.0f}% {time_rating}{quality_rating}")
        
        # Calculate improvement
        baseline_time = 11.3
        improvement = (baseline_time - avg_time) / baseline_time * 100
        
        print(f"\nğŸ‰ GPT-5-MINI æ”¹å–„çµæœ:")
        print(f"   æ™‚é–“: {baseline_time:.1f}ç§’ â†’ {avg_time:.1f}ç§’")
        print(f"   æ”¹å–„: {improvement:.1f}% é«˜é€ŸåŒ–")


if __name__ == "__main__":
    try:
        test_gpt5_mini_final()
    except KeyboardInterrupt:
        print("\nâš ï¸ Test interrupted")
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback
        traceback.print_exc()