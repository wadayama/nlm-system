#!/usr/bin/env python
"""Compare direct API call vs NLM system overhead"""

from time import time
from openai import OpenAI
from nlm_interpreter import NLMSession


def test_api_overhead():
    """Measure overhead of NLM system vs direct API calls"""
    print("="*70)
    print("ğŸ” API ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰åˆ†æ (gpt-5-nano)")
    print("="*70)
    
    # Load API key
    try:
        with open('.openai_key', 'r') as f:
            api_key = f.read().strip()
    except FileNotFoundError:
        print("âŒ .openai_key not found")
        return
    
    client = OpenAI(api_key=api_key)
    
    # Test 1: Direct API call (simplest possible)
    print("\n1ï¸âƒ£ ç›´æ¥APIå‘¼ã³å‡ºã—ï¼ˆæœ€å°æ§‹æˆï¼‰:")
    print("-" * 50)
    
    start = time()
    response = client.chat.completions.create(
        model="gpt-5-nano",
        messages=[
            {"role": "user", "content": "Say hello"}
        ]
    )
    direct_simple_time = time() - start
    print(f"  Message: 'Say hello'")
    print(f"  Response: {response.choices[0].message.content}")
    print(f"  â±ï¸  Time: {direct_simple_time:.3f}s")
    
    # Test 2: Direct API call with tools (like NLM)
    print("\n2ï¸âƒ£ ç›´æ¥APIå‘¼ã³å‡ºã—ï¼ˆãƒ„ãƒ¼ãƒ«å®šç¾©ä»˜ãï¼‰:")
    print("-" * 50)
    
    tools_definition = [
        {
            "type": "function",
            "function": {
                "name": "save_variable",
                "description": "Save a value to a variable",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "variable_name": {"type": "string"},
                        "value": {"type": "string"}
                    },
                    "required": ["variable_name", "value"]
                }
            }
        }
    ]
    
    start = time()
    response = client.chat.completions.create(
        model="gpt-5-nano",
        messages=[
            {"role": "system", "content": "You are a helpful assistant that can save variables."},
            {"role": "user", "content": "Say hello"}
        ],
        tools=tools_definition
    )
    direct_tools_time = time() - start
    print(f"  Message: 'Say hello' (with tools)")
    print(f"  Response: {response.choices[0].message.content}")
    print(f"  â±ï¸  Time: {direct_tools_time:.3f}s")
    
    # Test 3: NLM system (no variables)
    print("\n3ï¸âƒ£ NLMã‚·ã‚¹ãƒ†ãƒ çµŒç”±ï¼ˆå¤‰æ•°ãªã—ï¼‰:")
    print("-" * 50)
    
    session = NLMSession(model="gpt-5-nano", namespace="overhead_test")
    
    start = time()
    result = session.execute("Say hello")
    nlm_no_var_time = time() - start
    print(f"  Command: 'Say hello'")
    print(f"  Result: {result[:100]}...")
    print(f"  â±ï¸  Time: {nlm_no_var_time:.3f}s")
    
    # Test 4: NLM system (with variable)
    print("\n4ï¸âƒ£ NLMã‚·ã‚¹ãƒ†ãƒ çµŒç”±ï¼ˆå¤‰æ•°ã‚ã‚Šï¼‰:")
    print("-" * 50)
    
    start = time()
    result = session.execute("Set {{greeting}} to 'hello'")
    nlm_var_time = time() - start
    value = session.get("greeting")
    print(f"  Command: Set {{{{greeting}}}} to 'hello'")
    print(f"  Variable value: {value}")
    print(f"  â±ï¸  Time: {nlm_var_time:.3f}s")
    
    # Analysis
    print("\n\n" + "="*70)
    print("ğŸ“Š ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰åˆ†æ:")
    print("="*70)
    
    print(f"\nâ±ï¸  ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“:")
    print(f"  1. ç›´æ¥APIï¼ˆæœ€å°ï¼‰:        {direct_simple_time:.3f}s")
    print(f"  2. ç›´æ¥APIï¼ˆãƒ„ãƒ¼ãƒ«ä»˜ãï¼‰:  {direct_tools_time:.3f}s")
    print(f"  3. NLMï¼ˆå¤‰æ•°ãªã—ï¼‰:        {nlm_no_var_time:.3f}s")
    print(f"  4. NLMï¼ˆå¤‰æ•°ã‚ã‚Šï¼‰:        {nlm_var_time:.3f}s")
    
    print(f"\nğŸ“ˆ ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰è¨ˆç®—:")
    tools_overhead = direct_tools_time - direct_simple_time
    nlm_overhead = nlm_no_var_time - direct_tools_time
    var_overhead = nlm_var_time - nlm_no_var_time
    
    print(f"  ãƒ„ãƒ¼ãƒ«å®šç¾©ã®è¿½åŠ :     +{tools_overhead:.3f}s ({tools_overhead/direct_simple_time*100:.0f}%)")
    print(f"  NLMã‚·ã‚¹ãƒ†ãƒ :          +{nlm_overhead:.3f}s ({nlm_overhead/direct_tools_time*100:.0f}%)")
    print(f"  å¤‰æ•°æ“ä½œ:             +{var_overhead:.3f}s ({var_overhead/nlm_no_var_time*100:.0f}%)")
    
    print(f"\nğŸ’¡ åˆ†æçµæœ:")
    if nlm_overhead > 1.0:
        print(f"  ğŸ”´ NLMã‚·ã‚¹ãƒ†ãƒ ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒå¤§ãã„ï¼ˆ+{nlm_overhead:.1f}ç§’ï¼‰")
        print(f"     åŸå› : ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€ãƒ„ãƒ¼ãƒ«å®šç¾©ã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ§‹ç¯‰")
    elif nlm_overhead > 0.5:
        print(f"  ğŸŸ¡ NLMã‚·ã‚¹ãƒ†ãƒ ã«ä¸­ç¨‹åº¦ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ï¼ˆ+{nlm_overhead:.1f}ç§’ï¼‰")
    else:
        print(f"  ğŸŸ¢ NLMã‚·ã‚¹ãƒ†ãƒ ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã¯è»½å¾®ï¼ˆ+{nlm_overhead:.1f}ç§’ï¼‰")
    
    if direct_simple_time > 2.0:
        print(f"  âš ï¸  APIè‡ªä½“ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒé…ã„ï¼ˆ{direct_simple_time:.1f}ç§’ï¼‰")
        print(f"     ã“ã‚Œã¯OpenAIå´ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒä¸»å› ")


if __name__ == "__main__":
    try:
        test_api_overhead()
    except KeyboardInterrupt:
        print("\nâš ï¸ Test interrupted")
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback
        traceback.print_exc()